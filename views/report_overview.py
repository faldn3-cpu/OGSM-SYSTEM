import streamlit as st
import pandas as pd
from datetime import date, datetime, timedelta
import gspread
import time
from gspread.exceptions import APIError, SpreadsheetNotFound
import logging

# === è¨­å®š:ç³»çµ±åˆ†é é»‘åå–® ===
SYSTEM_SHEETS = [
    "DATA", "ç¶“éŠ·åƒ¹(ç¸½)", "æ•´å¥—æ­é…", "åƒæ•¸è¨­å®š", "ç¸½è¡¨", 
    "æº«æ§å™¨", "é›·å°„", "SENSOR", "æ¸›é€Ÿæ©Ÿ", "è®Šé »å™¨", "ä¼ºæœ", 
    "PLC", "äººæ©Ÿ", "è»Ÿé«”", "Robot", "é…ä»¶", "ç«¯å­è‡º",
    "Users", "Logs", "Sessions"
]

# === è¨­å®š:ç›´è³£å…¨å“¡åå–® ===
DIRECT_SALES_NAMES = [
    "æ›¾ä»å›", "æº«é”ä»", "æ¥Šå®¶è±ª", "èŠå¯Œä¸", "è¬ç‘é¨", "ä½•å®›èŒ¹", "å¼µæ›¸å‰"
]

# === è¨­å®š:ç¶“éŠ·å…¨å“¡åå–® ===
DISTRIBUTOR_SALES_NAMES = [
    "å¼µä½•é”", "å‘¨æŸç¿°", "è‘‰ä»è±ª"
]

# === è¨­å®š:ç‰¹æ®Šç¾¤çµ„é¸é …åç¨± ===
OPT_ALL = "(1) ğŸŸ¢ å…¨å“¡é¸å–"
OPT_DIRECT = "(2) ğŸ”µ ç›´è³£å…¨å“¡"
OPT_DIST = "(3) ğŸŸ  ç¶“éŠ·å…¨å“¡"
SPECIAL_OPTS = [OPT_ALL, OPT_DIRECT, OPT_DIST]

# === ã€æ–°å¢ã€‘è³‡æ–™åº«é€£ç·šå¿«å– ===
@st.cache_resource(ttl=600)  # å¿«å– 10 åˆ†é˜
def get_spreadsheet_with_retry(client, db_name, max_retries=3):
    """
    ä½¿ç”¨é‡è©¦æ©Ÿåˆ¶é–‹å•Ÿ Google Sheets (è§£æ±ºå¿«é€Ÿåˆ‡æ›æ™‚çš„é€£ç·šå•é¡Œ)
    """
    for attempt in range(max_retries):
        try:
            sh = client.open(db_name)
            logging.info(f"Successfully opened spreadsheet: {db_name}")
            return sh
        except SpreadsheetNotFound:
            # å¦‚æœçœŸçš„æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼Œç›´æ¥å ±éŒ¯
            logging.error(f"Spreadsheet not found: {db_name}")
            raise
        except APIError as e:
            if "429" in str(e) or "Quota exceeded" in str(e):
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    logging.warning(f"API 429 when opening spreadsheet, retry in {wait_time}s")
                    time.sleep(wait_time)
                    continue
                else:
                    logging.error(f"Failed to open spreadsheet after {max_retries} retries")
                    raise
            else:
                # å…¶ä»– API éŒ¯èª¤
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                else:
                    raise
        except Exception as e:
            # ç¶²è·¯æš«æ™‚æ€§éŒ¯èª¤
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 1.5
                logging.warning(f"Temporary error opening spreadsheet: {e}, retry in {wait_time}s")
                time.sleep(wait_time)
                continue
            else:
                logging.error(f"Failed to open spreadsheet: {e}")
                raise
    
    return None

# === ã€æ–°å¢ã€‘å·¥ä½œè¡¨åˆ—è¡¨å¿«å– ===
@st.cache_data(ttl=300)  # å¿«å– 5 åˆ†é˜
def get_worksheets_cached(_spreadsheet):
    """
    å¿«å–å·¥ä½œè¡¨åˆ—è¡¨ (æ¸›å°‘ API å‘¼å«)
    """
    try:
        worksheets = _spreadsheet.worksheets()
        return {ws.title: ws for ws in worksheets}
    except Exception as e:
        logging.error(f"Failed to get worksheets: {e}")
        return {}

# === CSV Injection é˜²è­· ===
def sanitize_csv_field(value):
    """æ¸…ç† CSV æ¬„ä½ä»¥é˜²æ³¨å…¥æ”»æ“Š"""
    if not isinstance(value, str):
        return value
    
    dangerous_chars = ['=', '+', '-', '@', '\t', '\r']
    if value and value[0] in dangerous_chars:
        return "'" + value
    
    return value

# === æ™ºæ…§å»¶é²ç­–ç•¥ ===
class APIRateLimiter:
    """API é€Ÿç‡é™åˆ¶å™¨ (æŒ‡æ•¸é€€é¿)"""
    def __init__(self):
        self.request_times = []
        self.base_delay = 0.5
        self.max_delay = 10
        self.current_delay = self.base_delay
        
    def wait(self):
        """æ™ºæ…§ç­‰å¾…"""
        now = time.time()
        self.request_times = [t for t in self.request_times if now - t < 60]
        
        if len(self.request_times) > 50:
            self.current_delay = min(self.current_delay * 1.5, self.max_delay)
        else:
            self.current_delay = max(self.current_delay * 0.9, self.base_delay)
        
        time.sleep(self.current_delay)
        self.request_times.append(now)
    
    def handle_error(self, attempt):
        """è™•ç† 429 éŒ¯èª¤çš„ç­‰å¾…æ™‚é–“"""
        wait_time = min(2 ** attempt * 2, 30)
        return wait_time

rate_limiter = APIRateLimiter()

def load_data_from_sheet(ws, start_date, end_date):
    """è®€å–è³‡æ–™ä¸¦æ¸…æ´— (åŠ å…¥é‡è©¦æ©Ÿåˆ¶)"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                wait_time = rate_limiter.handle_error(attempt)
                time.sleep(wait_time)
            
            data = ws.get_all_records()
            ui_columns = ["æ—¥æœŸ", "æ˜ŸæœŸ", "å®¢æˆ¶åç¨±", "å®¢æˆ¶åˆ†é¡", "å·¥ä½œå…§å®¹", "å¯¦éš›è¡Œç¨‹", "æœ€å¾Œæ›´æ–°æ™‚é–“"]
            
            if not data:
                return pd.DataFrame(columns=ui_columns)
            
            df = pd.DataFrame(data)

            if "é …æ¬¡" in df.columns:
                df = df.drop(columns=["é …æ¬¡"])
            
            for col in ui_columns:
                if col not in df.columns:
                    df[col] = ""

            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce').dt.date
            df = df.dropna(subset=["æ—¥æœŸ"]) 

            mask = (df["æ—¥æœŸ"] >= start_date) & (df["æ—¥æœŸ"] <= end_date)
            filtered_df = df.loc[mask].copy()
            
            filtered_df = filtered_df.sort_values(by=["æ—¥æœŸ"], ascending=False)
            return filtered_df[ui_columns]
        
        except APIError as e:
            if "429" in str(e) or "Quota exceeded" in str(e):
                if attempt < max_retries - 1:
                    wait_time = rate_limiter.handle_error(attempt + 1)
                    logging.warning(f"API 429 error, waiting {wait_time}s before retry")
                    continue
                else:
                    logging.error(f"API quota exceeded after {max_retries} retries")
                    raise
            else:
                logging.error(f"API error: {e}")
                raise
        except Exception as e:
            logging.error(f"Failed to load data from sheet: {e}")
            return pd.DataFrame()
    
    return pd.DataFrame()

def get_all_sales_names(ws_map):
    """å¾å·¥ä½œè¡¨å­—å…¸ä¸­ç¯©é¸æ¥­å‹™å“¡åç¨±"""
    sales_names = []
    for title in ws_map.keys():
        if title not in SYSTEM_SHEETS and not title.startswith("æ•´å¥—_") and "ç¶“éŠ·" not in title:
            sales_names.append(title)
    return sales_names

def show(client, db_name, user_email, real_name, is_manager):
    st.title("ğŸ“Š æ—¥å ±ç¸½è¦½èˆ‡åŒ¯å‡º")

    # === ã€ä¿®å¾©ã€‘ä½¿ç”¨é‡è©¦æ©Ÿåˆ¶é–‹å•Ÿè³‡æ–™åº« ===
    try:
        with st.spinner("æ­£åœ¨é€£ç·šè³‡æ–™åº«..."):
            sh = get_spreadsheet_with_retry(client, db_name)
            if not sh:
                st.error(f"âŒ ç„¡æ³•é–‹å•Ÿè³‡æ–™åº«: {db_name}")
                st.info("ğŸ’¡ è«‹ç¨å¾Œå†è©¦ï¼Œæˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡")
                return
    except SpreadsheetNotFound:
        st.error(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™åº«: {db_name}")
        st.info("ğŸ’¡ è«‹ç¢ºèªè³‡æ–™åº«åç¨±æ˜¯å¦æ­£ç¢ºï¼Œæˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡")
        logging.error(f"Spreadsheet not found: {db_name}")
        return
    except Exception as e:
        st.error(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—")
        st.info("ğŸ’¡ å¯èƒ½åŸå› ï¼š\n1. ç¶²è·¯ä¸ç©©å®š\n2. Google API æš«æ™‚æ€§éŒ¯èª¤\n3. è«‹é‡æ–°æ•´ç†é é¢")
        logging.error(f"Failed to open database: {e}")
        return

    # === 1. æ—¥æœŸé¸æ“‡å™¨ ===
    col1, col2 = st.columns([2, 1])
    with col1:
        today = date.today()
        start_default = today - timedelta(days=today.weekday())
        end_default = today
        
        date_range = st.date_input(
            "ğŸ“… é¸æ“‡æŸ¥è©¢å€é–“", 
            (start_default, end_default),
            key="overview_range_picker"
        )

    if isinstance(date_range, tuple) and len(date_range) == 2:
        start_date, end_date = date_range
    else:
        st.warning("è«‹é¸æ“‡å®Œæ•´çš„èµ·å§‹èˆ‡çµæŸæ—¥æœŸ")
        return

    # === 2. äººå“¡é¸æ“‡ ===
    user_role = "manager" if is_manager else "sales"
    current_user_name = real_name
    target_users = []

    # === ã€ä¿®å¾©ã€‘ä½¿ç”¨å¿«å–çš„å·¥ä½œè¡¨åˆ—è¡¨ ===
    try:
        with st.spinner("æ­£åœ¨è®€å–å·¥ä½œè¡¨åˆ—è¡¨..."):
            ws_map = get_worksheets_cached(sh)
            if not ws_map:
                st.error("âŒ ç„¡æ³•è®€å–å·¥ä½œè¡¨åˆ—è¡¨")
                return
    except Exception as e:
        st.error(f"âŒ è®€å–è³‡æ–™åº«çµæ§‹å¤±æ•—")
        st.info("ğŸ’¡ è«‹é‡æ–°æ•´ç†é é¢å¾Œå†è©¦")
        logging.error(f"Failed to load worksheets: {e}")
        return

    if user_role == "manager":
        if "overview_sales_select" not in st.session_state:
            st.session_state.overview_sales_select = []
        if "overview_sales_prev" not in st.session_state:
            st.session_state.overview_sales_prev = st.session_state.overview_sales_select

        all_sales = get_all_sales_names(ws_map)
        
        valid_direct_names = [name for name in DIRECT_SALES_NAMES if name in all_sales]
        valid_dist_names = [name for name in DISTRIBUTOR_SALES_NAMES if name in all_sales]
        
        menu_options = SPECIAL_OPTS + sorted(all_sales)

        def on_selection_change():
            current = st.session_state.overview_sales_select
            previous = st.session_state.overview_sales_prev
            
            added = [item for item in current if item not in previous]
            new_selection = current
            
            if added:
                new_item = added[-1]
                if new_item in SPECIAL_OPTS:
                    new_selection = [new_item]
                else:
                    new_selection = [item for item in current if item not in SPECIAL_OPTS]
            
            st.session_state.overview_sales_select = new_selection
            st.session_state.overview_sales_prev = new_selection

        with col2:
            st.multiselect(
                "ğŸ‘¥ é¸æ“‡æŸ¥çœ‹å°è±¡",
                options=menu_options,
                key="overview_sales_select", 
                on_change=on_selection_change 
            )

        selected_options = st.session_state.overview_sales_select
        
        final_target_set = set()
        if OPT_ALL in selected_options:
            final_target_set.update(all_sales)
        else:
            if OPT_DIRECT in selected_options:
                final_target_set.update(valid_direct_names)
            if OPT_DIST in selected_options:
                final_target_set.update(valid_dist_names)
            
            for opt in selected_options:
                if opt not in SPECIAL_OPTS:
                    final_target_set.add(opt)
        
        target_users = sorted(list(final_target_set))
            
    else:
        with col2:
            st.text_input("ğŸ‘¤ æŸ¥çœ‹å°è±¡", value=current_user_name, disabled=True)
        target_users = [current_user_name]

    if not target_users:
        if user_role == "manager":
            st.info("è«‹é¸æ“‡äººå“¡æˆ–ç¾¤çµ„ (é è¨­ä¸é¡¯ç¤ºï¼Œè«‹æ‰‹å‹•é¸æ“‡)ã€‚")
        else:
            st.error("æ‰¾ä¸åˆ°æ‚¨çš„è³‡æ–™è¡¨ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡ã€‚")
        return

    st.markdown("---")
    
    # === 3. è®€å–èˆ‡é¡¯ç¤º (æ™ºæ…§é€Ÿç‡é™åˆ¶ç‰ˆ) ===
    all_data = []
    
    MAX_USERS = 30
    if len(target_users) > MAX_USERS:
        st.error(f"âš ï¸ ä¸€æ¬¡æœ€å¤šæŸ¥è©¢ {MAX_USERS} ä½æ¥­å‹™å“¡ï¼Œè«‹ç¸®å°ç¯„åœ")
        st.info("ğŸ’¡ å»ºè­°ä½¿ç”¨ã€Œç›´è³£å…¨å“¡ã€æˆ–ã€Œç¶“éŠ·å…¨å“¡ã€ç¾¤çµ„ï¼Œæˆ–æ‰‹å‹•é¸æ“‡å°‘æ•¸äººå“¡")
        return
    
    estimated_time = len(target_users) * rate_limiter.current_delay
    st.info(f"â±ï¸ æ­£åœ¨è®€å– {len(target_users)} ä½æ¥­å‹™å“¡è³‡æ–™ (é è¨ˆéœ€æ™‚ {estimated_time:.1f} ç§’)")
    
    # === ã€æ–°å¢ã€‘é˜²æ­¢é‡è¤‡æŸ¥è©¢çš„æ©Ÿåˆ¶ ===
    query_key = f"{start_date}_{end_date}_{'_'.join(sorted(target_users))}"
    
    if "last_query_key" not in st.session_state:
        st.session_state.last_query_key = ""
    if "last_query_data" not in st.session_state:
        st.session_state.last_query_data = None
    
    # å¦‚æœæŸ¥è©¢æ¢ä»¶ç›¸åŒï¼Œç›´æ¥ä½¿ç”¨å¿«å–çµæœ
    if st.session_state.last_query_key == query_key and st.session_state.last_query_data is not None:
        st.success("âœ… ä½¿ç”¨å¿«å–è³‡æ–™ (ç„¡éœ€é‡æ–°æŸ¥è©¢)")
        final_df = st.session_state.last_query_data
    else:
        # åŸ·è¡Œæ–°æŸ¥è©¢
        with st.spinner(f"å½™æ•´ä¸­... (ä½¿ç”¨æ™ºæ…§é€Ÿç‡é™åˆ¶ä»¥é¿å…è¶…è¼‰)"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            failed_users = []
            
            for idx, user_name in enumerate(target_users):
                status_text.text(f"æ­£åœ¨è®€å–: {user_name} ({idx+1}/{len(target_users)})")
                ws = ws_map.get(user_name)
                
                if ws:
                    try:
                        rate_limiter.wait()
                        
                        df = load_data_from_sheet(ws, start_date, end_date)
                        if not df.empty:
                            df.insert(0, "æ¥­å‹™å“¡", user_name)
                            all_data.append(df)
                    
                    except APIError as e:
                        if "429" in str(e):
                            failed_users.append(user_name)
                            st.warning(f"âš ï¸ {user_name} è®€å–å¤±æ•— (API è¶…è¼‰)ï¼Œè«‹ç¨å¾Œé‡è©¦")
                            logging.error(f"API 429 for {user_name}")
                        else:
                            failed_users.append(user_name)
                            logging.error(f"API error for {user_name}: {e}")
                    
                    except Exception as e:
                        failed_users.append(user_name)
                        logging.error(f"Unexpected error loading {user_name}: {e}")
                
                progress_bar.progress((idx + 1) / len(target_users))
            
            status_text.empty()
            progress_bar.empty()
            
            if failed_users:
                st.error(f"âŒ ä»¥ä¸‹ {len(failed_users)} ä½æ¥­å‹™å“¡è³‡æ–™è®€å–å¤±æ•—: {', '.join(failed_users)}")
                st.info("ğŸ’¡ å»ºè­°: ç­‰å¾… 1 åˆ†é˜å¾Œé‡æ–°æŸ¥è©¢ï¼Œæˆ–æ¸›å°‘ä¸€æ¬¡æŸ¥è©¢çš„äººæ•¸")

        if not all_data:
            st.info("ğŸ” æ‰€é¸å€é–“å…§ç„¡è³‡æ–™ã€‚")
            return

        final_df = pd.concat(all_data, ignore_index=True)
        
        # å„²å­˜åˆ°å¿«å–
        st.session_state.last_query_key = query_key
        st.session_state.last_query_data = final_df
    
    # çµ±è¨ˆæ‘˜è¦
    st.subheader(f"ğŸ“ˆ çµ±è¨ˆæ‘˜è¦ ({start_date} ~ {end_date})")
    m1, m2, m3 = st.columns(3)
    m1.metric("ç¸½å¡«å¯«ç­†æ•¸", len(final_df))
    m2.metric("åƒèˆ‡æ¥­å‹™äººæ•¸", len(final_df["æ¥­å‹™å“¡"].unique()))
    m3.metric("æ‹œè¨ªå®¢æˆ¶æ•¸", len(final_df["å®¢æˆ¶åç¨±"].unique()))

    # è©³ç´°è¡¨æ ¼
    st.subheader("ğŸ“ è©³ç´°åˆ—è¡¨")
    
    MAX_DISPLAY_ROWS = 1000
    if len(final_df) > MAX_DISPLAY_ROWS:
        st.warning(f"âš ï¸ è³‡æ–™éå¤šï¼Œåƒ…é¡¯ç¤ºå‰ {MAX_DISPLAY_ROWS} ç­† (ä¸‹è¼‰ CSV å¯å–å¾—å®Œæ•´è³‡æ–™)")
        display_df = final_df.head(MAX_DISPLAY_ROWS)
    else:
        display_df = final_df
    
    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "æ—¥æœŸ": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
            "æœ€å¾Œæ›´æ–°æ™‚é–“": st.column_config.TextColumn("æ›´æ–°æ™‚é–“", width="small")
        }
    )

    # åŒ¯å‡º CSV
    fname = f"æ¥­å‹™æ—¥å ±å½™æ•´_{start_date}_{end_date}.csv"
    
    export_df = final_df.copy()
    export_df = export_df.applymap(sanitize_csv_field)
    
    csv = export_df.to_csv(index=False).encode('utf-8-sig')
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è¼‰ CSV å ±è¡¨",
        data=csv,
        file_name=fname,
        mime="text/csv",
        type="primary"
    )
    st.caption("âš ï¸ ä¸‹è¼‰å¾Œè«‹åœ¨å—ä¿¡ä»»çš„ç’°å¢ƒä¸­é–‹å•Ÿæª”æ¡ˆ")
    
    # === ã€æ–°å¢ã€‘æ‰‹å‹•æ¸…é™¤å¿«å–æŒ‰éˆ• ===
    st.markdown("---")
    if st.button("ğŸ”„ å¼·åˆ¶é‡æ–°æŸ¥è©¢ (æ¸…é™¤å¿«å–)", help="å¦‚æœè³‡æ–™æœ‰æ›´æ–°ä½†æœªé¡¯ç¤ºï¼Œè«‹é»æ­¤æŒ‰éˆ•"):
        st.session_state.last_query_key = ""
        st.session_state.last_query_data = None
        # åŒæ™‚æ¸…é™¤ Streamlit çš„ cache
        st.cache_data.clear()
        st.cache_resource.clear()
        st.success("âœ… å¿«å–å·²æ¸…é™¤ï¼Œé‡æ–°æ•´ç†é é¢ä»¥è¼‰å…¥æœ€æ–°è³‡æ–™")
        time.sleep(1)
        st.rerun()